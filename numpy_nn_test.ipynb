{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# === 資料預處理 ===\n",
        "# 載入 Fashion-MNIST 資料集\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# 正規化像素值到 0~1 範圍\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# 可選：分割驗證集（20% 訓練集作為驗證）\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# === 模型建構 ===\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),  # 輸入層展平\n",
        "    tf.keras.layers.Dense(256, activation='relu'),  # 隱藏層1\n",
        "    tf.keras.layers.Dense(128, activation='relu'),  # 隱藏層2\n",
        "    tf.keras.layers.Dense(10, activation='softmax') # 輸出層\n",
        "])\n",
        "\n",
        "# 編譯模型\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# === 模型訓練 ===\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=15,\n",
        "    batch_size=256,\n",
        "    validation_data=(x_val, y_val)\n",
        ")\n",
        "\n",
        "# === 模型評估 ===\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'\\n測試集準確率: {test_acc:.4f}')\n",
        "\n",
        "# === 儲存模型 ===\n",
        "# 儲存為 .h5 格式\n",
        "model.save('fashion_mnist.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiWSm4MCaSiZ",
        "outputId": "56fc12fd-6390-499e-ed5a-0b4346fba172"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7215 - loss: 0.8102 - val_accuracy: 0.8473 - val_loss: 0.4264\n",
            "Epoch 2/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8568 - loss: 0.4085 - val_accuracy: 0.8636 - val_loss: 0.3818\n",
            "Epoch 3/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8765 - loss: 0.3434 - val_accuracy: 0.8711 - val_loss: 0.3535\n",
            "Epoch 4/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.8853 - loss: 0.3175 - val_accuracy: 0.8706 - val_loss: 0.3514\n",
            "Epoch 5/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8879 - loss: 0.3048 - val_accuracy: 0.8805 - val_loss: 0.3304\n",
            "Epoch 6/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8961 - loss: 0.2847 - val_accuracy: 0.8832 - val_loss: 0.3210\n",
            "Epoch 7/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9030 - loss: 0.2642 - val_accuracy: 0.8724 - val_loss: 0.3535\n",
            "Epoch 8/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9009 - loss: 0.2637 - val_accuracy: 0.8852 - val_loss: 0.3237\n",
            "Epoch 9/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9072 - loss: 0.2485 - val_accuracy: 0.8846 - val_loss: 0.3185\n",
            "Epoch 10/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9113 - loss: 0.2342 - val_accuracy: 0.8900 - val_loss: 0.3120\n",
            "Epoch 11/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9139 - loss: 0.2287 - val_accuracy: 0.8889 - val_loss: 0.3172\n",
            "Epoch 12/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9221 - loss: 0.2087 - val_accuracy: 0.8893 - val_loss: 0.3111\n",
            "Epoch 13/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9220 - loss: 0.2092 - val_accuracy: 0.8904 - val_loss: 0.3098\n",
            "Epoch 14/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9198 - loss: 0.2081 - val_accuracy: 0.8938 - val_loss: 0.3051\n",
            "Epoch 15/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9276 - loss: 0.1978 - val_accuracy: 0.8946 - val_loss: 0.3089\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8860 - loss: 0.3293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "測試集準確率: 0.8881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_MODEL_NAME = 'fashion_mnist' # Default extension is h5\n",
        "TF_MODEL_PATH = f'{YOUR_MODEL_NAME}.h5'\n",
        "MODEL_WEIGHTS_PATH = f'{YOUR_MODEL_NAME}.npz'\n",
        "MODEL_ARCH_PATH = f'{YOUR_MODEL_NAME}.json'"
      ],
      "metadata": {
        "id": "R5kOVSijmRGF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(TF_MODEL_PATH)\n",
        "\n",
        "# Save weights to .npz (NumPy format)\n",
        "weights = model.get_weights()\n",
        "np.savez('model_weights.npz', *weights)\n",
        "\n",
        "# Save architecture to JSON\n",
        "with open('model_architecture.json', 'w') as json_file:\n",
        "    json_file.write(model.to_json())"
      ],
      "metadata": {
        "id": "4HeolTt5ND_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd900b2-7d1e-4b70-9a03-9aa4845d154e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# === Step 1: Load Keras .h5 model ===\n",
        "model = tf.keras.models.load_model(TF_MODEL_PATH)\n",
        "\n",
        "# === Step 2: Print and collect weights ===\n",
        "params = {}\n",
        "print(\"🔍 Extracting weights from model...\\n\")\n",
        "for layer in model.layers:\n",
        "    weights = layer.get_weights()\n",
        "    if weights:\n",
        "        print(f\"Layer: {layer.name}\")\n",
        "        for i, w in enumerate(weights):\n",
        "            param_name = f\"{layer.name}_{i}\"\n",
        "            print(f\"  {param_name}: shape={w.shape}\")\n",
        "            params[param_name] = w\n",
        "        print()\n",
        "\n",
        "# === Step 3: Save to .npz ===\n",
        "np.savez(MODEL_WEIGHTS_PATH, **params)\n",
        "print(f\"✅ Saved all weights to {MODEL_WEIGHTS_PATH}\")\n",
        "\n",
        "# === Step 4: Reload and verify ===\n",
        "print(\"\\n🔁 Verifying loaded .npz weights...\\n\")\n",
        "loaded = np.load(MODEL_WEIGHTS_PATH)\n",
        "\n",
        "for key in loaded.files:\n",
        "    print(f\"{key}: shape={loaded[key].shape}\")\n",
        "\n",
        "# === Step 6: Extract architecture to JSON ===\n",
        "arch = []\n",
        "for layer in model.layers:\n",
        "    config = layer.get_config()\n",
        "    info = {\n",
        "        \"name\": layer.name,\n",
        "        \"type\": layer.__class__.__name__,\n",
        "        \"config\": config,\n",
        "        \"weights\": [f\"{layer.name}_{i}\" for i in range(len(layer.get_weights()))]\n",
        "    }\n",
        "    arch.append(info)\n",
        "\n",
        "with open(MODEL_ARCH_PATH, \"w\") as f:\n",
        "    json.dump(arch, f, indent=2)\n",
        "\n",
        "print(\"✅ Architecture saved to model_architecture.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lqDQAtPl2Y5",
        "outputId": "e0fefb24-1266-4fa2-ef3d-fdee6690a66e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Extracting weights from model...\n",
            "\n",
            "Layer: dense\n",
            "  dense_0: shape=(784, 256)\n",
            "  dense_1: shape=(256,)\n",
            "\n",
            "Layer: dense_1\n",
            "  dense_1_0: shape=(256, 128)\n",
            "  dense_1_1: shape=(128,)\n",
            "\n",
            "Layer: dense_2\n",
            "  dense_2_0: shape=(128, 10)\n",
            "  dense_2_1: shape=(10,)\n",
            "\n",
            "✅ Saved all weights to fashion_mnist.npz\n",
            "\n",
            "🔁 Verifying loaded .npz weights...\n",
            "\n",
            "dense_0: shape=(784, 256)\n",
            "dense_1: shape=(256,)\n",
            "dense_1_0: shape=(256, 128)\n",
            "dense_1_1: shape=(128,)\n",
            "dense_2_0: shape=(128, 10)\n",
            "dense_2_1: shape=(10,)\n",
            "✅ Architecture saved to model_architecture.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN forward Path using Numpy only"
      ],
      "metadata": {
        "id": "XdPYPffGoJek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# === Load weights and architecture ===\n",
        "weights = np.load(MODEL_WEIGHTS_PATH)\n",
        "with open(MODEL_ARCH_PATH) as f:\n",
        "    architecture = json.load(f)\n",
        "\n",
        "\n",
        "# === Activation functions ===\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e / np.sum(e, axis=-1, keepdims=True)\n",
        "\n",
        "# === Flatten ===\n",
        "def flatten(x):\n",
        "    return x.reshape(x.shape[0], -1)\n",
        "\n",
        "# === Dense layer ===\n",
        "def dense(x, W, b):\n",
        "    return x @ W + b\n",
        "\n",
        "# === Forward pass ===\n",
        "def forward(x):\n",
        "    for layer in architecture:\n",
        "        lname = layer['name']\n",
        "        ltype = layer['type']\n",
        "        cfg = layer['config']\n",
        "        wnames = layer['weights']\n",
        "\n",
        "\n",
        "        if ltype == \"Flatten\":\n",
        "            x = flatten(x)\n",
        "\n",
        "        elif ltype == \"Dense\":\n",
        "            W = weights[wnames[0]]\n",
        "            b = weights[wnames[1]]\n",
        "            x = dense(x, W, b)\n",
        "            if cfg.get(\"activation\") == \"relu\":\n",
        "                x = relu(x)\n",
        "            elif cfg.get(\"activation\") == \"softmax\":\n",
        "                x = softmax(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# === Example usage ===\n",
        "# Load a dummy image (batch size 1)\n",
        "# Make sure it's shape: (1, 28, 28, 1)\n",
        "dummy_input = np.random.rand(1, 28*28).astype(np.float32)\n",
        "output = forward(dummy_input)\n",
        "\n",
        "print(\"🧠 Output probabilities:\", output)\n",
        "print(\"✅ Predicted class:\", np.argmax(output, axis=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXaoHjRxnd7B",
        "outputId": "2f67156d-16d2-49cb-ac4b-4a108db015d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Output probabilities: [[8.42138519e-08 8.97192390e-12 1.12395874e-07 2.86170776e-09\n",
            "  4.64359051e-10 2.10895309e-16 6.22713287e-07 2.26218042e-15\n",
            "  9.99999166e-01 3.48914756e-12]]\n",
            "✅ Predicted class: [8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34RfS1ZNOMzb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}